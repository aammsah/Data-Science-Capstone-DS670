---
title: "Kaggle Titanic"
author: "Kapil Bastola"
date: "2/18/2017"
output: pdf_document
---
  
  ```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown

This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:
  
  ```{r}
train <- read.csv('~/Downloads/train (2).csv', header = T)
test <- read.csv('~/Downloads/test (2).csv', header = TRUE)
target<- read.csv('~/Downloads/gender_submission.csv', header = TRUE)
str(train)
summary(train)
summary(test)
summary(target)


table(train$Survived)
table(train$Sex)
prop.table(table(train$Sex, train$Survived),1)
table(train$Pclass)
prop.table(table(train$Pclass, train$Survived),1)
hist(train$Age)
```


```{r, message=FALSE, warning=FALSE}
library(rpart)
library(rattle)
library(rpart.plot)
library(RColorBrewer)
library(SDMTools)
library(ROCR)
library(randomForest)
library(e1071)
library(caret)
```

##Decision Trees

```{r}
fit <- rpart(Survived ~ Pclass + Sex + Age + SibSp + Parch + Fare + Embarked,
             data=train,
             method="class")

fancyRpartPlot(fit)
prediction_decision.tree<- predict(fit, test, type = "class")
confusion.matrix(prediction_decision.tree, target[[2]])

#create function to measure precision, recall and f-measure
measurePrecisionRecall <- function(predict, target_data){
  precision <- sum(predict & target_data) / sum(predict)
  recall <- sum(predict & target_data) / sum(target_data)
  fmeasure <- 2 * precision * recall / (precision + recall)
  
  cat('precision:  ')
  cat(precision * 100)
  cat('%')
  cat('\n')
  
  cat('recall:     ')
  cat(recall * 100)
  cat('%')
  cat('\n')
  
  cat('f-measure:  ')
  cat(fmeasure * 100)
  cat('%')
  cat('\n')
}

measurePrecisionRecall(as.numeric(as.character(prediction_decision.tree)), target[[2]])

```

Notice that Age field has many NAs. We use decision tree to predict Age and fill in the blank values. Also there is 1 value for Fare that is null. We use median fare to fill in that blank value. Embarked field has two null values and we use S as the value because that has the most number of Embarked.

```{r}
#first we combine the train and test datasets to engineer the datasets together
test$Survived <- NA
combined_data <- rbind(train,test)
str(combined_data)



Agefit <- rpart(Age ~ Pclass + Sex + SibSp + Parch + Fare + Embarked,
                data=combined_data[!is.na(combined_data$Age),], 
                method="anova")
combined_data$Age[is.na(combined_data$Age)] <- predict(Agefit, combined_data[is.na(combined_data$Age),])


summary(combined_data$Embarked)
which(combined_data$Embarked=='')
combined_data$Embarked[c(62,830)]= "S"
combined_data$Embarked <- factor(combined_data$Embarked)
summary(combined_data$Embarked)

summary(combined_data$Fare)
which(is.na(combined_data$Fare))
combined_data$Fare[1044]<-median(combined_data$Fare, na.rm = T)

#Next we breakdown datasets into original train and test sets
train2 <- combined_data[1:891,]
test2 <- combined_data[892:1309,]
```

I redid the decision trees with the modified dataset.

```{r}
fit2 <- rpart(Survived ~ Pclass + Sex + Age + SibSp + Parch + Fare + Embarked,
              data=train2,
              method="class")
fancyRpartPlot(fit)
pred2<- predict(fit, test2, type = "class")
confusion.matrix(pred2, target[[2]])
measurePrecisionRecall(as.numeric(as.character(pred2)), target[[2]])

pred_dt <- ROCR::prediction(as.numeric(as.character(pred2)), target[[2]])
ROC1 <- performance(pred_dt, measure = "tpr", x.measure = "fpr")
plot(ROC1, col = "orange", lwd = 2)
auc1 <- slot(performance(pred_dt, "auc"), 'y.values')
auc1
```
##Random Forest
```{r}

set.seed(455)
fit3 <- randomForest(as.factor(Survived) ~ Pclass +Sex + SibSp + Parch + Embarked + Age +Fare,
                     data = train2, importance = TRUE, ntree = 10000)
summary(fit3)
varImpPlot(fit3)
pred3<-predict(fit3, test2)
confusion.matrix(pred3, target[[2]])
measurePrecisionRecall(as.numeric(as.character(pred3)), target[[2]])

pred_RF <- ROCR::prediction(as.numeric(as.character(pred3)), target[[2]])
ROC2 <- performance(pred_RF, measure = "tpr", x.measure = "fpr")
plot(ROC2, col = "red", lwd = 2)
auc2 <- slot(performance(pred_RF, "auc"), 'y.values')
auc2
```
##Support Vector Machines

Since SVM do not work well with categorical values, we created dummy variables for our categorical variables. These variables were Sex and Embarked.

```{r}
combined_data$Sex_dummy <- ifelse (combined_data$Sex == "female", 1, 2)
combined_data$Embarked_dummy <- ifelse(combined_data$Embarked == "C",1, 
                                       ifelse(combined_data$Embarked == "Q", 2, 3))
train2 <- combined_data[1:891,]
test2 <- combined_data[892:1309,]
y_train <- as.factor(train2[,2])
x_train <- train2[,c(-1,-2,-4,-5, -9,-11, -12)]

fit4<- svm(x_train,y_train,cost = 100, gamma =1)
pred4 <- predict(fit4,test2[,c(-1,-2,-4,-5, -9,-11, -12)])
summary(pred4)
confusion.matrix(pred4, target[[2]])
measurePrecisionRecall(as.numeric(as.character(pred4)), target[[2]])

pred_SVM <- ROCR::prediction(as.numeric(as.character(pred4)), target[[2]])
ROC3 <- performance(pred_SVM, measure = "tpr", x.measure = "fpr")
plot(ROC3, col = "blue", lwd = 2)
auc3 <- slot(performance(pred_SVM, "auc"), 'y.values')
auc3
```


```{r}



nnet <- train(Survived~Pclass +Sex_dummy + SibSp + Parch + 
                Embarked_dummy + Age +Fare, data = train2, 
              method = 'nnet', linout = TRUE, trace = FALSE)
pred5<- predict(nnet, test2[,c(-1,-2,-4,-5, -9,-11, -12)])
pred5 <- ifelse(pred5 > 0.5, 1, 0)
confusion.matrix(pred5, target[[2]])
measurePrecisionRecall(as.numeric(as.character(pred5)), target[[2]])

pred_NN <- ROCR::prediction(as.numeric(as.character(pred5)), target[[2]])
ROC4 <- performance(pred_NN, measure = "tpr", x.measure = "fpr")
plot(ROC4, col = "green", lwd = 2)
auc4 <- slot(performance(pred_NN, "auc"), 'y.values')
auc4

```
```{r}
logit_reg <- glm(Survived ~ Pclass + Sex + Age + SibSp + Pclass:Sex, family = binomial, data = train2)
summary(logit_reg)
pred6 <- predict(logit_reg, test2, type = "response")
head(pred6)
pred6 <- ifelse(pred6 > 0.5, 1, 0)
confusion.matrix(pred6, target[[2]])
measurePrecisionRecall(as.numeric(as.character(pred6)), target[[2]])

pred_LR <- ROCR::prediction(as.numeric(as.character(pred6)), target[[2]])
ROC5 <- performance(pred_LR, measure = "tpr", x.measure = "fpr")
plot(ROC5, col = "brown", lwd = 2)
auc5 <- slot(performance(pred_LR, "auc"), 'y.values')
auc5

```

##Evaluation

```{r}
DT.Eval <- c("DT",95.38,81.58,87.94,85.15)
RF.Eval <- c("RF",87.31, 76.97,81.82, 85.15 )
SVM.Eval <- c("SVM",80.43,73.03, 76.55, 81.44 )
NN.Eval <- c("ANN",91.55,92.76,92.15,93.94)
LR.Eval <- c("LR",96.32,86.18,90.97,92.15)
df.Eval <- rbind.data.frame(DT.Eval, RF.Eval, SVM.Eval, NN.Eval, LR.Eval)
names(df.Eval) <- c("Method","Precision", "Recall", "f-measure", "auc")
df.Eval
```
We see that Neural networks has the best performance in all metrics except precision.

##Prediction
```{r}
pred5.df<- data.frame(pred5)
write.csv(pred5.df, file = "predicted_output")
```
